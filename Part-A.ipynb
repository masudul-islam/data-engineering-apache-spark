{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df87c04c-0a78-49a1-b9d7-a246484a8438",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API\n",
    "## Question A.1\n",
    "### A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87f269e-ad3f-4e1a-9c72-72a9b1fae04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 09:23:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the English transcripts: 406934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 09:30:14 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/02/21 09:30:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.250:7077\") \\\n",
    "    .appName(\"MasudulIslam\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.port\", \"9999\") \\\n",
    "    .config(\"spark.blockManager.port\", \"10005\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize Spark Context from the Spark Session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define the base path for the HDFS directory containing the transcripts\n",
    "base_path = \"hdfs://192.168.2.250:9000/europarl/\"\n",
    "\n",
    "# Read the English transcripts and count the number of lines\n",
    "eng_transcripts_rdd = sc.textFile(base_path + \"europarl-v7.bg-en.en\")\n",
    "eng_line_count = eng_transcripts_rdd.count()\n",
    "print(\"Number of lines in the English transcripts:\", eng_line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271815dd-20cf-44ee-bcee-ad08eda8fe25",
   "metadata": {},
   "source": [
    "### A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff177c4c-61f2-4d30-aacf-9ab9b93117b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the Bulgarian transcripts: 406934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the Bulgarian transcripts and count the number of lines\n",
    "bg_transcripts_rdd = sc.textFile(base_path + \"europarl-v7.bg-en.bg\")\n",
    "bg_line_count = bg_transcripts_rdd.count()\n",
    "print(\"Number of lines in the Bulgarian transcripts:\", bg_line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ec900-b791-4bab-9472-ebc8d634b199",
   "metadata": {},
   "source": [
    "### A.1.3 Verify that the line counts are the same for the two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bef7e33-cc0f-407d-955f-5b0ec373d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line counts are the same for both languages.\n"
     ]
    }
   ],
   "source": [
    "if eng_line_count == bg_line_count:\n",
    "    print(\"The line counts are the same for both languages.\")\n",
    "else:\n",
    "    print(\"The line counts are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f129b-5d47-4b02-90fb-9db6e681d87a",
   "metadata": {},
   "source": [
    "### A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc7305f-c4dc-46ed-a625-ae2ff5e8ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the English RDD: 2\n",
      "Number of partitions in the Bulgarian RDD: 2\n"
     ]
    }
   ],
   "source": [
    "# Count the number of partitions for the English transcripts\n",
    "eng_partitions = eng_transcripts_rdd.getNumPartitions()\n",
    "print(\"Number of partitions in the English RDD:\", eng_partitions)\n",
    "\n",
    "# Count the number of partitions for the Bulgarian transcripts\n",
    "bg_partitions = bg_transcripts_rdd.getNumPartitions()\n",
    "print(\"Number of partitions in the Bulgarian RDD:\", bg_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1681b5-9c31-4148-8728-fb17e1a2dc3b",
   "metadata": {},
   "source": [
    "## Question A.2\n",
    "### A.2.1 Pre-process the text from both RDDs by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c6ff5b-15f2-40a8-b54b-21803c3155ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to lowercase and tokenize the text\n",
    "def preprocess_line(line):\n",
    "    return line.lower().split()\n",
    "\n",
    "# Apply the function to both RDDs\n",
    "eng_processed_rdd = eng_transcripts_rdd.map(preprocess_line)\n",
    "bg_processed_rdd = bg_transcripts_rdd.map(preprocess_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4d808-0f8e-44e8-b26a-a8db0996d8f3",
   "metadata": {},
   "source": [
    "### A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f715380-f476-433f-abc1-7c5f9a6ebba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the English processed RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['membership', 'of', 'parliament:', 'see', 'minutes'], ['approval', 'of', 'minutes', 'of', 'previous', 'sitting:', 'see', 'minutes'], ['membership', 'of', 'parliament:', 'see', 'minutes'], ['verification', 'of', 'credentials:', 'see', 'minutes'], ['documents', 'received:', 'see', 'minutes'], ['written', 'statements', 'and', 'oral', 'questions', '(tabling):', 'see', 'minutes'], ['petitions:', 'see', 'minutes'], ['texts', 'of', 'agreements', 'forwarded', 'by', 'the', 'council:', 'see', 'minutes'], ['action', 'taken', 'on', \"parliament's\", 'resolutions:', 'see', 'minutes'], ['agenda', 'for', 'next', 'sitting:', 'see', 'minutes']]\n",
      "Sample from the Bulgarian processed RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['състав', 'на', 'парламента:', 'вж.', 'протоколи'], ['одобряване', 'на', 'протокола', 'от', 'предишното', 'заседание:', 'вж', 'протоколите'], ['състав', 'на', 'парламента:', 'вж.', 'протоколи'], ['проверка', 'на', 'пълномощията:', 'вж.', 'протоколи'], ['внасяне', 'на', 'документи:', 'вж.', 'протоколи'], ['въпроси', 'с', 'искане', 'за', 'устен', 'отговор', 'и', 'писмени', 'декларации', '(внасяне):', 'вж.', 'протокола'], ['петиции:', 'вж.', 'протоколи'], ['предаване', 'на', 'текстове', 'на', 'споразумения', 'от', 'съвета:', 'вж.', 'протоколи'], ['действия,', 'предприети', 'вследствие', 'резолюции', 'на', 'парламента:', 'вж.', 'протокола'], ['дневен', 'ред', 'на', 'следващото', 'заседание:', 'вж.', 'протоколи']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inspect 10 entries from the English processed RDD\n",
    "print(\"Sample from the English processed RDD:\")\n",
    "print(eng_processed_rdd.take(10))\n",
    "\n",
    "# Inspect 10 entries from the Bulgarian processed RDD\n",
    "print(\"Sample from the Bulgarian processed RDD:\")\n",
    "print(bg_processed_rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374e6d6-c738-44cc-8aa8-85e75eca185b",
   "metadata": {},
   "source": [
    "### A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21554ec3-6e4d-45b8-8a5c-e1d064389ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line counts still match after pre-processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify that the line counts still match after pre-processing\n",
    "assert eng_processed_rdd.count() == bg_processed_rdd.count(), \"Line counts differ after pre-processing\"\n",
    "print(\"Line counts still match after pre-processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6faa589-b162-4a73-a5c5-9c29bb52c0e9",
   "metadata": {},
   "source": [
    "## Question A.3\n",
    "### A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a35262e-6781-4b63-865e-1fcc010b8646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the English corpus: [('the', 752615), ('of', 363742), ('to', 328387), ('and', 294459), ('in', 240247), ('a', 166056), ('that', 162448), ('is', 158054), ('for', 123519), ('we', 110215)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the Bulgarian corpus: [('на', 604938), ('да', 330186), ('и', 328079), ('за', 261271), ('в', 228108), ('от', 168749), ('се', 150472), ('е', 129681), ('че', 114145), ('с', 95262)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the 10 most frequent words in the English corpus\n",
    "eng_word_counts = eng_processed_rdd.flatMap(lambda line: line).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "top_10_eng_words = eng_word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(\"Top 10 words in the English corpus:\", top_10_eng_words)\n",
    "\n",
    "# Compute the 10 most frequent words in the Bulgarian corpus\n",
    "bg_word_counts = bg_processed_rdd.flatMap(lambda line: line).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "top_10_bg_words = bg_word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(\"Top 10 words in the Bulgarian corpus:\", top_10_bg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf303f-1cd5-40a4-82a7-a0b1668dd15e",
   "metadata": {},
   "source": [
    "### A.3.2 Verify that your results are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c533fa0e-6c76-4474-aeaa-7acfae8f03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 English words and their counts:\n",
      "the: 752615\n",
      "of: 363742\n",
      "to: 328387\n",
      "and: 294459\n",
      "in: 240247\n",
      "a: 166056\n",
      "that: 162448\n",
      "is: 158054\n",
      "for: 123519\n",
      "we: 110215\n",
      "\n",
      "Top 10 Bulgarian words and their counts:\n",
      "на: 604938\n",
      "да: 330186\n",
      "и: 328079\n",
      "за: 261271\n",
      "в: 228108\n",
      "от: 168749\n",
      "се: 150472\n",
      "е: 129681\n",
      "че: 114145\n",
      "с: 95262\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 words from English and Bulgarian corpuses\n",
    "print(\"Top 10 English words and their counts:\")\n",
    "for word, count in top_10_eng_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Bulgarian words and their counts:\")\n",
    "for word, count in top_10_bg_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7ca3c-84e3-43a5-9211-211921227011",
   "metadata": {},
   "source": [
    "## Question A.4\n",
    "### A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d030feb9-3672-49b6-855f-ed4a09e9ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent translation pairs:\n",
      "('(applause)', '(ръкопляскания)'): 911\n",
      "('written', 'писмени'): 687\n",
      "('is', 'е'): 626\n",
      "('(rule', '(член'): 566\n",
      "('of', 'на'): 492\n",
      "('statements', 'изявления'): 457\n",
      "('149)', '149)'): 423\n",
      "('see', 'вж.'): 349\n",
      "('thank', 'благодаря'): 298\n",
      "('that', 'това'): 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Key the lines by their line number\n",
    "eng_indexed = eng_processed_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "bg_indexed = bg_processed_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# 2. Swap the key and value - so that the line number is the key\n",
    "# (This step is already accomplished by how we structured the zipWithIndex() result.\n",
    "\n",
    "# 3. Join the two RDDs together according to the line number key, so we have pairs of matching lines\n",
    "paired_sentences = eng_indexed.join(bg_indexed)\n",
    "\n",
    "# 4. Filter to exclude line pairs that have an empty/missing \"corresponding\" sentence\n",
    "non_empty_pairs = paired_sentences.filter(lambda x: x[1][0] and x[1][1])\n",
    "\n",
    "# 5. Filter to leave only pairs of sentences with a small number of words per sentence\n",
    "# (Here, let's choose sentences with 5 or fewer words as \"small\")\n",
    "small_sentence_pairs = non_empty_pairs.filter(lambda x: len(x[1][0]) <= 5 and len(x[1][1]) <= 5)\n",
    "\n",
    "# 6. Filter to leave only pairs of sentences with the same number of words in each sentence\n",
    "equal_length_pairs = small_sentence_pairs.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "\n",
    "# 7. For each sentence pair, map so that we pair each (in order) word in the two sentences\n",
    "word_pairs = equal_length_pairs.flatMap(lambda x: zip(x[1][0], x[1][1]))\n",
    "\n",
    "# 8. Use reduce to count the number of occurrences of the word-translation-pairs\n",
    "word_pair_counts = word_pairs.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 9. Print some of the most frequently occurring pairs of words\n",
    "most_frequent_word_pairs = word_pair_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(\"Most frequent translation pairs:\")\n",
    "for pair, count in most_frequent_word_pairs:\n",
    "    print(f\"{pair}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
